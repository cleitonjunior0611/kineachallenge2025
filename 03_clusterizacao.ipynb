{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "478c4bfd-9562-4e80-a6bc-0a6045facfd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Clusterização dos Prospectos de Fundos de Investimentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fb00dc2-88f8-4536-ab27-50a5ee6110de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Pipeline Tratamento Dados LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78c01a10-fb81-436a-b660-ecfd42bf5598",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "A classe abaixo: (i) carrega o dataframe com os dados extraídos via LLM, (ii) inclui coluna classificação ANBIMA, (iii) inclui historico cotacao bolsa, (iv) filtra por tipo de fundo e (v) integra dados LLM Kinea e Concorrência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "113b5a6f-0738-46d6-986e-05171c144b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install yfinance pandas numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "74df8b67-ef40-430e-911c-6393ea6f2eef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import regexp_replace, to_date, when, col, coalesce, lit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from pyspark.sql.types import StringType, DateType, FloatType\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "class DataPipeline:\n",
    "    def __init__(self, spark):\n",
    "        self.spark = spark\n",
    "        self.df = None\n",
    "        self.taxa_rf_diaria = None  # para armazenar taxa SELIC diária\n",
    "\n",
    "    def load_and_merge(self, df1: DataFrame, df2: DataFrame):\n",
    "        if \"tipo\" in df1.columns:\n",
    "            df1 = df1.withColumnRenamed(\"tipo\", \"tipo_fundo\")\n",
    "        if \"tipo\" in df2.columns:\n",
    "            df2 = df2.withColumnRenamed(\"tipo\", \"tipo_fundo\")\n",
    "        self.df = df1.unionByName(df2)\n",
    "        return self\n",
    "\n",
    "    def standardize_columns(self):\n",
    "        self.df = (\n",
    "            self.df\n",
    "            .withColumn(\"cnpj\", regexp_replace(\"cnpj\", \"[^0-9]\", \"\"))\n",
    "            .withColumn(\"data_emissao\", to_date(\"data_emissao\", \"yyyy-MM-dd\"))\n",
    "            .withColumn(\"tipo_fundo\", when(col(\"tipo_fundo\") == \"F.I.I.\", \"FII\").otherwise(col(\"tipo_fundo\")))\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def add_anbima_classification(self, anbima_table: str):\n",
    "        df_anbima = (\n",
    "            self.spark.table(anbima_table)\n",
    "            .select(col(\"identificador_classe\").alias(\"cnpj\"), col(\"tipo_anbima\"))\n",
    "        )\n",
    "        self.df = self.df.join(df_anbima, on=\"cnpj\", how=\"left\")\n",
    "        return self\n",
    "\n",
    "    def add_ticker(self, ticker_table: str):\n",
    "        df_ticker = (\n",
    "            self.spark.table(ticker_table)\n",
    "            .select(col(\"Ticker\"), col(\"CNPJ\").alias(\"cnpj\"))\n",
    "            .withColumn(\"cnpj\", regexp_replace(\"cnpj\", \"[^0-9]\", \"\"))\n",
    "        )\n",
    "        self.df = self.df.join(df_ticker, on=\"cnpj\", how=\"left\")\n",
    "        return self\n",
    "\n",
    "    def drop_duplicates(self):\n",
    "        self.df = self.df.dropDuplicates([\"cnpj\", \"data_emissao\"])\n",
    "        return self\n",
    "\n",
    "    def carregar_taxa_selic(self):\n",
    "        \"\"\"Busca taxa SELIC média diária do último ano.\"\"\"\n",
    "        hoje = datetime.today()\n",
    "        inicio = hoje - timedelta(days=365)\n",
    "        selic_url = (\n",
    "            f\"https://api.bcb.gov.br/dados/serie/bcdata.sgs.1178/dados\"\n",
    "            f\"?formato=csv&dataInicial={inicio.strftime('%d/%m/%Y')}\"\n",
    "            f\"&dataFinal={hoje.strftime('%d/%m/%Y')}\"\n",
    "        )\n",
    "        try:\n",
    "            selic_df = pd.read_csv(selic_url, sep=';', decimal=',', parse_dates=['data'], dayfirst=True)\n",
    "            selic_df.columns = ['Data', 'SELIC']\n",
    "            selic_df['SELIC'] = pd.to_numeric(selic_df['SELIC'], errors='coerce')\n",
    "            selic_df.dropna(inplace=True)\n",
    "            taxa_rf_anual = selic_df['SELIC'].mean() / 100\n",
    "            self.taxa_rf_diaria = (1 + taxa_rf_anual) ** (1/252) - 1\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] Falha ao buscar SELIC: {e}\")\n",
    "            self.taxa_rf_diaria = 0.0  # fallback para evitar crash\n",
    "\n",
    "    def calcular_metricas(self, ticker):\n",
    "        if not isinstance(ticker, str) or ticker.strip() == '':\n",
    "            return pd.Series([np.nan]*5, index=[\n",
    "                'volatilidade_historica', 'liquidez_media', 'drawdown_max',\n",
    "                'retorno_acumulado', 'sharpe_ratio'\n",
    "            ])\n",
    "\n",
    "        ticker_yf = ticker.strip()\n",
    "        if not ticker_yf.endswith('.SA'):\n",
    "            ticker_yf += \".SA\"\n",
    "\n",
    "        try:\n",
    "            dados = yf.download(ticker_yf, period=\"1y\", interval=\"1d\", progress=False, auto_adjust=True)\n",
    "            if dados.empty:\n",
    "                return pd.Series([np.nan]*5, index=[\n",
    "                    'volatilidade_historica', 'liquidez_media', 'drawdown_max',\n",
    "                    'retorno_acumulado', 'sharpe_ratio'\n",
    "                ])\n",
    "\n",
    "            close = dados['Close']\n",
    "            volume = dados['Volume']\n",
    "            retorno_diario = close.pct_change().dropna()\n",
    "\n",
    "            # garante float escalar\n",
    "            volatilidade = float(retorno_diario.std() * np.sqrt(252))\n",
    "            liquidez = float(volume.mean())\n",
    "            retorno_acumulado = float((close.iloc[-1] / close.iloc[0]) - 1)\n",
    "            drawdown_max = float(((close - close.cummax()) / close.cummax()).min())\n",
    "\n",
    "            taxa_rf = float(self.taxa_rf_diaria or 0)\n",
    "            excesso_retorno_diario = retorno_diario - taxa_rf\n",
    "            sharpe_ratio = float((excesso_retorno_diario.mean() * 252) / volatilidade) if volatilidade != 0 else np.nan\n",
    "\n",
    "            return pd.Series([\n",
    "                volatilidade, liquidez, drawdown_max, retorno_acumulado, sharpe_ratio\n",
    "            ], index=[\n",
    "                'volatilidade_historica', 'liquidez_media', 'drawdown_max',\n",
    "                'retorno_acumulado', 'sharpe_ratio'\n",
    "            ])\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[ERRO] Falha ao calcular métricas para {ticker_yf}: {e}\")\n",
    "            return pd.Series([np.nan]*5, index=[\n",
    "                'volatilidade_historica', 'liquidez_media', 'drawdown_max',\n",
    "                'retorno_acumulado', 'sharpe_ratio'\n",
    "            ])\n",
    "\n",
    "    def add_market_metrics(self):\n",
    "        \"\"\"Adiciona métricas de mercado para cada ticker (inclui Sharpe Ratio).\"\"\"\n",
    "        # Pega SELIC uma vez só\n",
    "        self.carregar_taxa_selic()\n",
    "\n",
    "        tickers = [\n",
    "            row['Ticker']\n",
    "            for row in self.df.select('Ticker').filter(col(\"Ticker\").isNotNull()).distinct().collect()\n",
    "            if str(row['Ticker']).strip() != ''\n",
    "        ]\n",
    "\n",
    "        lista_metricas = []\n",
    "        for t in tickers:\n",
    "            serie = self.calcular_metricas(t)\n",
    "            d = serie.to_dict()\n",
    "            d['Ticker'] = t\n",
    "            lista_metricas.append(d)\n",
    "\n",
    "        df_metricas = pd.DataFrame(lista_metricas)\n",
    "        for col_name in ['volatilidade_historica', 'liquidez_media', 'drawdown_max', 'retorno_acumulado', 'sharpe_ratio']:\n",
    "            df_metricas[col_name] = df_metricas[col_name].astype(float)\n",
    "        df_metricas['Ticker'] = df_metricas['Ticker'].astype(str)\n",
    "        df_metricas = df_metricas.where(pd.notnull(df_metricas), None)\n",
    "\n",
    "        df_metricas_spark = self.spark.createDataFrame(df_metricas)\n",
    "        self.df = self.df.join(df_metricas_spark, on='Ticker', how='left')\n",
    "        return self\n",
    "    \n",
    "    def add_valor_cotado_atual(self):\n",
    "        \"\"\"\n",
    "        Calcula o valor cotado atual de cada fundo com Ticker\n",
    "        e adiciona uma coluna 'valor_cotado_atual' no DataFrame.\n",
    "        \"\"\"\n",
    "\n",
    "        # Lista de tickers únicos\n",
    "        tickers = [\n",
    "            row['Ticker']\n",
    "            for row in self.df.select('Ticker').filter(col(\"Ticker\").isNotNull()).distinct().collect()\n",
    "            if str(row['Ticker']).strip() != ''\n",
    "        ]\n",
    "\n",
    "        # Busca valores cotados atuais\n",
    "        lista_valores = []\n",
    "        for t in tickers:\n",
    "            ticker_yf = t.strip()\n",
    "            if not ticker_yf.endswith(\".SA\"):\n",
    "                ticker_yf += \".SA\"\n",
    "            try:\n",
    "                dados = yf.download(ticker_yf, period=\"5d\", interval=\"1d\", progress=False, auto_adjust=True)\n",
    "                if dados.empty:\n",
    "                    valor_atual = None\n",
    "                else:\n",
    "                    valor_atual = float(dados['Close'].iloc[-1])\n",
    "                lista_valores.append({\"Ticker\": t, \"valor_cotado_atual\": valor_atual})\n",
    "            except Exception as e:\n",
    "                print(f\"[ERRO] Falha ao obter valor atual para {t}: {e}\")\n",
    "                lista_valores.append({\"Ticker\": t, \"valor_cotado_atual\": None})\n",
    "\n",
    "        # Cria DataFrame Spark com valores cotados\n",
    "        df_valores_spark = self.spark.createDataFrame(pd.DataFrame(lista_valores))\n",
    "\n",
    "        # Faz join com o DataFrame principal\n",
    "        self.df = self.df.join(df_valores_spark, on=\"Ticker\", how=\"left\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def filter_by_type(self, tipo):\n",
    "        return self.df.filter(self.df[\"tipo_fundo\"] == tipo)\n",
    "\n",
    "    def get_df(self):\n",
    "        return self.df\n",
    "    \n",
    "    def cast_fundo_columns(self):\n",
    "        schema_cast = {\n",
    "            \"tipo_fundo\": StringType(),\n",
    "            \"cnpj\": StringType(),\n",
    "            \"data_emissao\": DateType(),\n",
    "            \"qt_emissoes\": FloatType(),\n",
    "            \"nome_fundo\": StringType(),\n",
    "            \"valor_cota_emissao\": FloatType(),\n",
    "            \"direito_preferencia_sobras_montante_adicional\": StringType(),\n",
    "            \"taxa_distribuicao_emissao\": FloatType(),\n",
    "            \"tabela_ativos_fundo\": StringType(),\n",
    "            \"sumario_experiencia_socios\": StringType(),\n",
    "            \"quantidade_cotas_emissao\": FloatType(),\n",
    "            \"quantidade_cotas_adicionais_emissao\": FloatType(),\n",
    "            \"publico_alvo\": StringType(),\n",
    "            \"obs_publico_alvo\": StringType(),\n",
    "            \"procuracao_AGE\": StringType(),\n",
    "            \"planilha_custos\": StringType(),\n",
    "            \"ordenar_fatores_risco\": StringType(),\n",
    "            \"montante_minimo_emissao\": FloatType(),\n",
    "            \"investimento_minimo_cpf_cnpj\": FloatType(),\n",
    "            \"investimento_minimo_inst\": FloatType(),\n",
    "            'investimento_maximo_cpf_cnpj': FloatType(),\n",
    "            'investimento_maximo_inst': FloatType(),\n",
    "            'percentual_oferta_institucional': FloatType(),\n",
    "            'volume_base_emissao': FloatType()\n",
    "        }\n",
    "\n",
    "        for col_name, dtype in schema_cast.items():\n",
    "            if col_name in self.df.columns:\n",
    "                self.df = self.df.withColumn(col_name, col(col_name).cast(dtype))\n",
    "\n",
    "        if \"quantidade_cotas_emissao\" in self.df.columns and \"quantidade_cotas_adicionais_emissao\" in self.df.columns:\n",
    "            self.df = self.df.withColumn(\n",
    "                \"quantidade_cotas_totais\",\n",
    "                coalesce(col(\"quantidade_cotas_emissao\"), lit(0)) + coalesce(col(\"quantidade_cotas_adicionais_emissao\"), lit(0))\n",
    "            )\n",
    "        return self\n",
    "    \n",
    "    def analisar_potencial_nova_emissao(self):\n",
    "            \"\"\"\n",
    "            Para fundos com ticker, verifica se valor_cotado_atual > valor_cota_emissao.\n",
    "            Adiciona coluna 'potencial_nova_emissao' (True/False).\n",
    "            \"\"\"\n",
    "            self.df = self.df.withColumn(\n",
    "                \"potencial_nova_emissao\",\n",
    "                F.when(\n",
    "                    (F.col(\"Ticker\").isNotNull()) & (F.col(\"valor_cotado_atual\") > F.col(\"valor_cota_emissao\")),\n",
    "                    True\n",
    "                ).otherwise(False)\n",
    "            )\n",
    "            return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bef4aab-13eb-4f22-bac9-4c489b0e73cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Aplicando a classe para os tipos de fundos trabalhados (FIDC, FII e FIP):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "16899a0c-3831-4785-aa34-278752011bb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df1 = spark.table(\"desafio_kinea.prospecto_fundos.extracao_prospectos_kinea_v3\")\n",
    "df2 = spark.table(\"desafio_kinea.prospecto_fundos.extracao_prospectos_v3\")\n",
    "\n",
    "# adicionando colunas artificiais - alterar futuramente\n",
    "df1 = df1.withColumn(\"tipo_gestor\", lit(\"KINEA\"))\n",
    "df2 = df2.withColumn(\"tipo_gestor\", lit(\"CONCORRENTE\"))\n",
    "\n",
    "pipeline = DataPipeline(spark)\n",
    "\n",
    "df_final = (\n",
    "    pipeline\n",
    "    .load_and_merge(df1, df2)\n",
    "    .standardize_columns()\n",
    "    .cast_fundo_columns()\n",
    "    .add_anbima_classification(\"desafio_kinea.prospecto_fundos.anbima_dev\")\n",
    "    .add_ticker(\"desafio_kinea.prospecto_fundos.mercado_fiis\")\n",
    "    .drop_duplicates()\n",
    "    .add_market_metrics()\n",
    "    .add_valor_cotado_atual()\n",
    "    .analisar_potencial_nova_emissao()\n",
    "    .get_df()  \n",
    ")\n",
    "\n",
    "df_fii = df_final.filter(col(\"tipo_fundo\") == \"FII\")\n",
    "df_fip = df_final.filter(col(\"tipo_fundo\") == \"FIP\")\n",
    "df_fidc = df_final.filter(col(\"tipo_fundo\") == \"FIDC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bb42930e-b67c-4b19-bf3e-2eeb9aa847d0",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755181218391}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_fii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f47409ab-a576-497e-a645-a33676e28f5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Modelo de Clusterização "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "25540f5f-686a-4338-9dce-f7f4daa4d518",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0a02c1b2-3d41-48e6-bbdf-30abbce72a3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, coalesce, lit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import warnings\n",
    "\n",
    "# Suprime warnings específicos do threadpoolctl\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='threadpoolctl')\n",
    "\n",
    "class ClusterizadorFundos:\n",
    "    def __init__(self, cols_numericas, cols_categoricas, modelo_embeddings='paraphrase-MiniLM-L6-v2'):\n",
    "        # Inicializa atributos com listas de colunas e modelo de embeddings\n",
    "        self.cols_numericas = cols_numericas\n",
    "        self.cols_categoricas = cols_categoricas\n",
    "        self.model = SentenceTransformer(modelo_embeddings)\n",
    "        self.scaler = RobustScaler()\n",
    "        self.pca = None\n",
    "        self.kmeans = None\n",
    "        self.embedding_cols = []\n",
    "        self.df_final = None\n",
    "\n",
    "    def _processar_categoricas(self, df_spark):\n",
    "        \"\"\"\n",
    "        Processa as colunas categóricas do DataFrame Spark:\n",
    "        - Seleciona as colunas categóricas e trata valores nulos.\n",
    "        - Gera embeddings para cada coluna categórica usando SentenceTransformer.\n",
    "        - Reduz a dimensionalidade dos embeddings com PCA.\n",
    "        - Retorna um DataFrame pandas com as colunas de embeddings.\n",
    "        \"\"\"\n",
    "        df_cat = df_spark.select('cnpj', 'data_emissao', 'tipo_fundo', *self.cols_categoricas)\n",
    "        for col_name in self.cols_categoricas:\n",
    "            df_cat = df_cat.withColumn(col_name, coalesce(col(col_name), lit('desconhecido')))\n",
    "        df_cat_pd = df_cat.toPandas().drop_duplicates(subset=['cnpj', 'data_emissao'])\n",
    "\n",
    "        embedding_cols_local = []\n",
    "        # Gera embeddings para cada coluna categórica e reduz dimensionalidade com PCA\n",
    "        for col_name in self.cols_categoricas:\n",
    "            texts = df_cat_pd[col_name].astype(str).fillna('desconhecido').tolist()\n",
    "            embeddings = self.model.encode(texts, batch_size=32, show_progress_bar=False)\n",
    "            pca_emb = PCA(n_components=min(10, embeddings.shape[1]), random_state=42)\n",
    "            emb_reduced = pca_emb.fit_transform(embeddings)\n",
    "            for i in range(emb_reduced.shape[1]):\n",
    "                col_emb_name = f'{col_name}_emb_{i}'\n",
    "                df_cat_pd[col_emb_name] = emb_reduced[:, i]\n",
    "                embedding_cols_local.append(col_emb_name)\n",
    "        self.embedding_cols = embedding_cols_local\n",
    "\n",
    "        # Retorna DataFrame sem as colunas categóricas originais\n",
    "        return df_cat_pd.drop(columns=self.cols_categoricas)\n",
    "\n",
    "    def preparar_dados(self, df_spark):\n",
    "        \"\"\"\n",
    "        Prepara os dados para clusterização:\n",
    "        - Preenche valores nulos e converte colunas numéricas para float.\n",
    "        - Processa colunas categóricas e numéricas, mesclando ambas.\n",
    "        - Normaliza os dados e aplica PCA para redução de dimensionalidade.\n",
    "        - Retorna DataFrame mesclado e matriz reduzida pelo PCA.\n",
    "        \"\"\"\n",
    "        df_spark = df_spark.fillna(0)\n",
    "        for c in self.cols_numericas:\n",
    "            df_spark = df_spark.withColumn(c, col(c).cast(\"float\"))\n",
    "\n",
    "        # Processa colunas categóricas e numéricas, mesclando ambas\n",
    "        df_cat_pd = self._processar_categoricas(df_spark)\n",
    "        df_num_pd = df_spark.select(['cnpj', 'data_emissao', 'tipo_fundo'] + self.cols_numericas).toPandas()\n",
    "        df_merged = df_num_pd.merge(df_cat_pd, on=['cnpj', 'data_emissao', 'tipo_fundo'], how='left')\n",
    "\n",
    "        # Preenche valores nulos nas colunas de interesse\n",
    "        for col_name in self.cols_numericas + self.embedding_cols:\n",
    "            if col_name in df_merged.columns:\n",
    "                df_merged[col_name] = df_merged[col_name].fillna(0)\n",
    "\n",
    "        # Normaliza os dados e aplica PCA para redução de dimensionalidade\n",
    "        X = df_merged[self.cols_numericas + self.embedding_cols].values\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "\n",
    "        n_features = X_scaled.shape[1]\n",
    "        if n_features < 2:\n",
    "            n_components_pca = n_features\n",
    "        else:\n",
    "            n_components_pca = 0.9\n",
    "        self.pca = PCA(n_components=n_components_pca, random_state=42)\n",
    "        X_pca = self.pca.fit_transform(X_scaled)\n",
    "\n",
    "        return df_merged, X_pca\n",
    "\n",
    "    def escolher_k_automatico(self, X_pca, k_min=2, k_max=10, tipo_fundo=None):\n",
    "        \"\"\"\n",
    "        Determina automaticamente o número ótimo de clusters (k):\n",
    "        - Calcula SSE (inércia) e Silhouette Score para diferentes valores de k.\n",
    "        - Plota gráficos do método do cotovelo e silhueta.\n",
    "        - Retorna o valor de k considerado ótimo.\n",
    "        \"\"\"\n",
    "        sse = []\n",
    "        sil_scores = []\n",
    "        k_range = range(k_min, k_max + 1)\n",
    "\n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            labels = kmeans.fit_predict(X_pca)\n",
    "            sse.append(kmeans.inertia_)\n",
    "            sil_scores.append(silhouette_score(X_pca, labels))\n",
    "\n",
    "        # Plota gráficos do método do cotovelo e silhueta\n",
    "        plt.figure(figsize=(12,5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(k_range, sse, marker='o')\n",
    "        titulo_cotovelo = \"Método do Cotovelo\"\n",
    "        if tipo_fundo:\n",
    "            titulo_cotovelo += f\" - {tipo_fundo}\"\n",
    "        plt.title(titulo_cotovelo)\n",
    "        plt.xlabel(\"Número de Clusters (k)\")\n",
    "        plt.ylabel(\"SSE (Inércia)\")\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(k_range, sil_scores, marker='o', color='orange')\n",
    "        titulo_silhueta = \"Coeficiente de Silhueta\"\n",
    "        if tipo_fundo:\n",
    "            titulo_silhueta += f\" - {tipo_fundo}\"\n",
    "        plt.title(titulo_silhueta)\n",
    "        plt.xlabel(\"Número de Clusters (k)\")\n",
    "        plt.ylabel(\"Silhouette Score\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Define k ótimo com base nos métodos do cotovelo e silhueta\n",
    "        k_otimo_inercia = k_range[np.argmin(np.gradient(np.gradient(sse)))]\n",
    "        k_otimo_sil = k_range[np.argmax(sil_scores)]\n",
    "\n",
    "        if k_otimo_inercia == k_otimo_sil:\n",
    "            return k_otimo_inercia\n",
    "        else:\n",
    "            return round((k_otimo_inercia + k_otimo_sil) / 2)\n",
    "\n",
    "    def aplicar_kmeans(self, df_merged, X_pca, k, tipo_fundo=None):\n",
    "        \"\"\"\n",
    "        Aplica o algoritmo KMeans para clusterização:\n",
    "        - Ajusta o modelo KMeans com o número de clusters k.\n",
    "        - Adiciona colunas de identificação de cluster ao DataFrame.\n",
    "        - Plota os clusters se houver mais de uma dimensão.\n",
    "        - Retorna o DataFrame com os clusters atribuídos.\n",
    "        \"\"\"\n",
    "        self.kmeans = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
    "        clusters = self.kmeans.fit_predict(X_pca)\n",
    "        df_merged['cluster_id'] = clusters\n",
    "        df_merged['cluster_id_full'] = df_merged['tipo_fundo'].astype(str) + \"_\" + df_merged['cluster_id'].astype(str)\n",
    "        \n",
    "        # Plota os clusters se houver mais de uma dimensão\n",
    "        if X_pca.shape[1] > 1:\n",
    "            plt.figure(figsize=(8,6))\n",
    "            sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=clusters, palette='Set1', s=60)\n",
    "            titulo_cluster = f\"Clusters K-Means (k={k}) + PCA + Embeddings\"\n",
    "            if tipo_fundo:\n",
    "                titulo_cluster += f\" - {tipo_fundo}\"\n",
    "            plt.title(titulo_cluster)\n",
    "            plt.xlabel(\"PC1\")\n",
    "            plt.ylabel(\"PC2\")\n",
    "            plt.legend(title='Cluster')\n",
    "            plt.show()\n",
    "        \n",
    "        return df_merged\n",
    "\n",
    "    def clusterizar_todos_tipos(self, df_spark, tipos, k_por_tipo=None):\n",
    "        \"\"\"\n",
    "        Realiza a clusterização para cada tipo de fundo especificado:\n",
    "        - Para cada tipo, prepara os dados, define k (manual ou automaticamente), aplica KMeans.\n",
    "        - Retorna um dicionário com os resultados em pandas e Spark DataFrame para cada tipo.\n",
    "        \"\"\"\n",
    "        resultados = {}\n",
    "        for tipo in tipos:\n",
    "            try:\n",
    "                df_tipo = df_spark.filter(df_spark[\"tipo_fundo\"] == tipo)\n",
    "                df_merge, X_pca = self.preparar_dados(df_tipo)\n",
    "\n",
    "                # Define k manualmente ou automaticamente\n",
    "                if k_por_tipo and tipo in k_por_tipo:\n",
    "                    k = k_por_tipo[tipo]\n",
    "                else:\n",
    "                    k = self.escolher_k_automatico(X_pca, tipo_fundo=tipo)\n",
    "\n",
    "                df_result = self.aplicar_kmeans(df_merge, X_pca, k, tipo_fundo=tipo)\n",
    "                resultados[tipo] = df_result\n",
    "\n",
    "                # Adiciona cluster_id e cluster_id_full ao DataFrame Spark de cada tipo\n",
    "                df_tipo_pd = df_result[['cnpj', 'data_emissao', 'cluster_id', 'cluster_id_full']]\n",
    "                df_tipo_spark = df_tipo.join(\n",
    "                    spark.createDataFrame(df_tipo_pd),\n",
    "                    on=['cnpj', 'data_emissao'],\n",
    "                    how='left'\n",
    "                )\n",
    "                resultados[f\"{tipo}_spark\"] = df_tipo_spark\n",
    "\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        return resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a458492f-e657-4e87-9d5b-6f560ac7b559",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.1. Parâmetros de Clusterização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9e51a836-aef8-44d7-87bd-4efa11999d19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista de colunas numéricas relevantes para clusterização dos fundos\n",
    "cols_num = [\n",
    "    'valor_cota_emissao',                  # Valor da cota na emissão\n",
    "    'taxa_distribuicao_emissao',           # Taxa de distribuição na emissão\n",
    "    'quantidade_cotas_totais',             # Quantidade de cotas totais emitidas\n",
    "    'investimento_minimo_cpf_cnpj',        # Investimento mínimo para CPF/CNPJ\n",
    "    'investimento_minimo_inst',            # Investimento mínimo para institucionais\n",
    "    'investimento_maximo_cpf_cnpj',        # Investimento máximo para CPF/CNPJ\n",
    "    'investimento_maximo_inst',            # Investimento máximo para institucionais\n",
    "    'percentual_oferta_institucional',     # Percentual da oferta para institucionais\n",
    "    'montante_minimo_emissao',             # Montante mínimo da emissão\n",
    "    'volume_base_emissao',                 # Volume base da emissão\n",
    "    'chamada_capital_ipca',                # Chamada de capital indexada ao IPCA\n",
    "    'volatilidade_historica', \n",
    "    'liquidez_media', \n",
    "    'drawdown_max', \n",
    "    'retorno_acumulado', \n",
    "    'sharpe_ratio'\n",
    "]\n",
    "\n",
    "# Lista de colunas categóricas relevantes para clusterização dos fundos\n",
    "cols_cat = [\n",
    "    'tabela_ativos_fundo',                 # Tabela de ativos do fundo\n",
    "    'publico_alvo',                        # Público alvo do fundo\n",
    "    'ordenar_fatores_risco',               # Fatores de risco ordenados\n",
    "    'diluicao_economica_novas_emissoes',   # Diluição econômica em novas emissões\n",
    "    'criterio_rateio',                     # Critério de rateio\n",
    "    'tipo_anbima'                          # Tipo ANBIMA do fundo\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf5b440d-3727-4edf-b83f-5fe25f079a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2. Aplicando Classe de Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "e2232ee7-0df0-44fb-8b28-cd7e48992977",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicializa o clusterizador com colunas numéricas e categóricas\n",
    "clusterizador = ClusterizadorFundos(cols_num, cols_cat)\n",
    "\n",
    "# Define os tipos de fundos a serem clusterizados\n",
    "tipos_fundos = ['FIDC', 'FII', 'FIP']\n",
    "\n",
    "# Define manualmente o número de clusters para cada tipo de fundo (ajustado conforme Elbow e Silluette Methods)\n",
    "# Caso contrário, o modelo define automaticamente a média das metodologias\n",
    "k_manual = {'FII': 5, 'FIDC': 5, 'FIP': 5}\n",
    "\n",
    "# Realiza a clusterização para cada tipo de fundo usando os valores de k definidos\n",
    "resultados = clusterizador.clusterizar_todos_tipos(df_final, tipos_fundos, k_por_tipo=k_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "4e68ebb7-595a-40bc-a3be-fae95664b3b4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "2": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1755092995851}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 2
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for tipo in tipos_fundos:\n",
    "    print(f'Visualizando: {tipo}')\n",
    "    display(resultados[tipo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b27b358f-9c2a-4ee1-ab37-9632805cfd27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Salvando os resultados no Schema\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "def select_cluster_cols(df_cluster):\n",
    "    \"\"\"Seleciona apenas as colunas relevantes para o join de clusters.\"\"\"\n",
    "    return df_cluster.select(\"cnpj\", \"data_emissao\", \"cluster_id\", \"cluster_id_full\")\n",
    "\n",
    "def bind_clusters(df_base, df_cluster):\n",
    "    \"\"\"Faz join entre o DataFrame base e o de clusterização.\"\"\"\n",
    "    return df_base.join(\n",
    "        select_cluster_cols(df_cluster),\n",
    "        on=[\"cnpj\", \"data_emissao\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "# Lista de tipos de fundo a serem processados\n",
    "tipos_fundos = ['FIDC', 'FII', 'FIP']\n",
    "\n",
    "dfs_bind = {}\n",
    "for tipo in tipos_fundos:\n",
    "    # Recupera o DataFrame base para o tipo de fundo\n",
    "    df_base = globals()[f\"df_{tipo.lower()}\"]\n",
    "    # Recupera o DataFrame de clusters para o tipo de fundo\n",
    "    df_cluster = resultados[f\"{tipo}_spark\"]\n",
    "\n",
    "    # Realiza o join entre o DataFrame base e o de clusters\n",
    "    df_bind = bind_clusters(df_base, df_cluster)\n",
    "    dfs_bind[tipo] = df_bind\n",
    "\n",
    "    # Cria variável global para o DataFrame resultante do join\n",
    "    globals()[f\"df_{tipo.lower()}_bind\"] = df_bind\n",
    "\n",
    "    # Salva o DataFrame resultante no schema especificado, substituindo se já existir\n",
    "    tabela = f\"desafio_kinea.prospecto_fundos.resultados_cluster_{tipo.lower()}\"\n",
    "    df_bind.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(tabela)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_clusterizacao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
